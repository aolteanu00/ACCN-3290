{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b8b242",
   "metadata": {},
   "source": [
    "# Asynchrounous Tech Assignment\n",
    "\n",
    "The top half of this document walks you through simple HTML web scraping techniques. Follow along with the video. After you have finished the video, complete the assignment at the bottom of this notebook. Answer the questions on Canvas and upload your saved workbook to receive full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7b753",
   "metadata": {},
   "source": [
    "### Simple HTML Web Scraping\n",
    "The objective of this example is to find one link on a webpage that you wish to extract information from. In this example, we will be identifying the total number of databases available on data.gov. You could easily adapt this example to find a CSV file on a webpage and download it, or some other simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6db83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: python\n"
     ]
    }
   ],
   "source": [
    "#Need to install this for HTML identification using CSS styles\n",
    "!pip install cssselect\n",
    "!pip install bs4\n",
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b837dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#package for accessing a website and retrieving its HTML source file\n",
    "import requests\n",
    "\n",
    "#package for parsing HTML - parsing means that Python will recognize HTML tags for you\n",
    "from lxml import html\n",
    "\n",
    "#alternative parsing package\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "\n",
    "#package for regular expressions - more on that later, but think of wildcard searches\n",
    "import re\n",
    "\n",
    "#importing a natural language processing package to count words and sentences\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ca010",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "Get the HTML source file from the website you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://www.data.gov/')\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd6900",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "Have Python parse (interpret) the HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#letting Python do the HTML parsing\n",
    "doc = html.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d947ba8",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "Identify the specific thing you are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = doc.cssselect('small a')[0]\n",
    "print(link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acc7a7",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "Develop a regular expression (pattern matching, wildcard matching). You can use this website to see how your regular expression will work as long as you provide it example text: https://pythex.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "rxDatasets = re.compile(r\"(?P<number>\\b\\d{0,3},?\\d{0,3},?\\d{1,3}\\b)\",\n",
    "    re.IGNORECASE | re.DOTALL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ec3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "numDataSets = rxDatasets.search(link.text).group(\"number\")\n",
    "print(numDataSets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f84232",
   "metadata": {},
   "source": [
    "### How do to this over multiple websites\n",
    "We will now turn to the task of scraping Wikipedia articles and generating simple word, number, and sentence counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc98635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with 2 wikipedia articles\n",
    "websites = [\"https://en.wikipedia.org/wiki/Enron\",\n",
    "           \"https://en.wikipedia.org/wiki/Enron_scandal\"]\n",
    "\n",
    "#making a blank dataframe to store our results\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "#rx for number of 1 digit numbers\n",
    "rxOneDig = re.compile(r\"\\b\\d\\b\",\n",
    "    re.IGNORECASE | re.DOTALL,\n",
    ")\n",
    "\n",
    "#for loop for our websites\n",
    "for url in websites:\n",
    "    resp = requests.get(url)\n",
    "    document = html.fromstring(resp.text)\n",
    "    \n",
    "    #identifying just the body of the article by its HTML element ID\n",
    "    body = document.get_element_by_id(\"mw-content-text\")\n",
    "    \n",
    "    #Using beautiful soup to parse HTML and return just text\n",
    "    actual_text = bsoup(html.tostring(body),'html').text\n",
    "    #print(actual_text)\n",
    "    \n",
    "    #Using a natural language processor for number of words and sentences\n",
    "    nlp_document = nlp(actual_text)\n",
    "    num_words=len(nlp_document)\n",
    "    num_sent=len(list(nlp_document.sents))\n",
    "    #print(num_words, num_sent)\n",
    "    \n",
    "    #using regular expression to find number of one digit numbers\n",
    "    num_OneDig = len(rxOneDig.findall(actual_text))\n",
    "    \n",
    "    #storing into final df\n",
    "    df_results = df_results.append({\n",
    "         \"link\": url,\n",
    "         \"numberWords\": num_words,\n",
    "         \"num1Dig\": num_OneDig,\n",
    "         \"numSents\": num_sent\n",
    "     }, ignore_index=True)\n",
    "    \n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf55563",
   "metadata": {},
   "source": [
    "# Assignment to turn in on Canvas\n",
    "Using the code in the cell above, modify that code to scrape the 5 wikipedia articles listed below.\n",
    "\n",
    "You should build a dataframe that stores the \"link\", the \"numberWords\",the number of *1 or 2* digit numbers \"num1or2Dig\", and the \"numSents\". Using this dataframe and descriptive (summary) statistics, answer the questions on Canvas. Make sure you save and upload this executed notebook to receive full credit.\n",
    "\n",
    "Hint: for one OR two digit numbers, you should modify the regular expression rxOneDig such that it searches for one or two consecutive digits surrounded by a boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685cdad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 wikipedia articles to scrape\n",
    "websites = [\"https://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act\",\n",
    "           \"https://en.wikipedia.org/wiki/Dodd%E2%80%93Frank_Wall_Street_Reform_and_Consumer_Protection_Act\",\n",
    "            \"https://en.wikipedia.org/wiki/Michael_Lewis\",\n",
    "            \"https://en.wikipedia.org/wiki/Tulane_University\",\n",
    "           \"https://en.wikipedia.org/wiki/Mardi_Gras_in_New_Orleans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ecda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c95be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b395deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
